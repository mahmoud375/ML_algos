{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Preprocessing for Student Performance, Breast Cancer, and Penguins Datasets\n",
    "\n",
    "\n",
    "\n",
    " This notebook preprocesses three datasets:\n",
    "\n",
    "\n",
    "\n",
    " - **Student_Performance.csv**: A regression dataset predicting `Performance Index`.\n",
    "\n",
    " - **breast-cancer.csv**: A classification dataset predicting `diagnosis`. Two target versions are produced:\n",
    "\n",
    "    - For logistic regression: target labels {0, 1}.\n",
    "\n",
    "    - For SVM: target labels {-1, 1}.\n",
    "\n",
    " - **penguins.csv**: A clustering dataset for a K-Means task. In this section, best practices are followed by:\n",
    "\n",
    "    - Handling missing values using the \"differentiated\" strategy (numeric columns are imputed with the median, categorical with the mode).\n",
    "\n",
    "    - Separating the ground‐truth label (`sex`) from the clustering features.\n",
    "\n",
    "    - Normalizing the numeric features (in the range [0,1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Setup\n",
    "\n",
    " Import necessary libraries and set up the project root for file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import custom utility functions from data_utils.py\n",
    "from src.scratch.utils.data_utils import (\n",
    "    load_data, \n",
    "    shuffle_data_pandas, \n",
    "    encode_categorical, \n",
    "    handle_missing_values, \n",
    "    feature_target_split, \n",
    "    normalize, \n",
    "    split_data,\n",
    "    drop_columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Student Performance Preprocessing (Regression)\n",
    "\n",
    "\n",
    "\n",
    " This section prepares the `Student_Performance.csv` dataset for predicting the `Performance Index`.\n",
    "\n",
    " Steps include shuffling, encoding categorical columns, handling missing values, feature‐target splitting, normalization, and splitting into training/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load and Inspect Data\n",
    "\n",
    " Load the dataset and display basic information to confirm structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Student Performance dataset and shuffle it\n",
    "data_path = Path(\"../data/raw/Regression_Dataset/Student_Performance.csv\")\n",
    "df_student = load_data(data_path)\n",
    "df_student = shuffle_data_pandas(df_student)\n",
    "\n",
    "# Display basic information and the first few rows\n",
    "print(\"Student Performance Dataset Info:\")\n",
    "print(df_student.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_student.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Encode Categorical Columns\n",
    "\n",
    "\n",
    "\n",
    " Here the `Extracurricular Activities` column (with values 'Yes'/'No') is encoded to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student = encode_categorical(df_student)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Handle Missing Values\n",
    "\n",
    " - From `df.info()`, there are no missing values (10,000 non-null entries per column).\n",
    "\n",
    " - Apply the function for completeness and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student = handle_missing_values(df_student, strategy=\"mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split Features and Target\n",
    "\n",
    " - **Target**: `Performance Index` (float64, continuous for regression).\n",
    "\n",
    " - **Features**: All other columns (`Hours Studied`, `Previous Scores`, `Extracurricular Activities`, `Sleep Hours`, `Sample Question Papers Practiced`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"Performance Index\"\n",
    "X_student, y_student = feature_target_split(df_student, target_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Features and Convert to NumPy Arrays\n",
    "\n",
    "- Normalize only the feature columns (X) to ensure consistent scale.\n",
    "\n",
    "- Do not normalize the target (`Performance Index`) as it’s a regression output.\n",
    "\n",
    "- Convert features and target to NumPy arrays for compatibility with machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_student = normalize(X_student)\n",
    "X_student = X_student.to_numpy()\n",
    "y_student = y_student.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split into Training and Test Sets and Save\n",
    "\n",
    "\n",
    "\n",
    " The data is split 80/20 for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_student, X_test_student, y_train_student, y_test_student = split_data(X_student, y_student, test_size=0.2)\n",
    "\n",
    "np.save(\"../data/processed/student_X_train.npy\", X_train_student)\n",
    "np.save(\"../data/processed/student_X_test.npy\", X_test_student)\n",
    "np.save(\"../data/processed/student_y_train.npy\", y_train_student)\n",
    "np.save(\"../data/processed/student_y_test.npy\", y_test_student)\n",
    "\n",
    "print(\"\\nStudent Performance data processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Breast Cancer Preprocessing (Classification)\n",
    "\n",
    "\n",
    "\n",
    " This section prepares the `breast-cancer.csv` dataset for predicting the `diagnosis` label.\n",
    "\n",
    " Two target versions are produced:\n",
    "\n",
    "\n",
    "\n",
    " - **Logistic Regression Version**: Targets remain {0,1} (‘M’ is mapped to 1 and ‘B’ to 0).\n",
    "\n",
    " - **SVM Version**: Targets are transformed to {-1,1} (with 0 converted to -1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load and Inspect Data\n",
    "\n",
    " Load the dataset and display basic information to confirm structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset and shuffle it\n",
    "data_path = Path(\"../data/raw/Classification_Dataset/breast-cancer.csv\")\n",
    "df_bc = load_data(data_path)\n",
    "df_bc = shuffle_data_pandas(df_bc)\n",
    "\n",
    "# Display basic info and first few rows\n",
    "print(\"Breast Cancer Dataset Info:\")\n",
    "print(df_bc.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_bc.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Irrelevant Columns and Encode Target\n",
    "\n",
    "- `id` column is irrelevant for modeling and should be removed.\n",
    "- `diagnosis` is the target column (object type, 'M' for malignant, 'B' for benign).\n",
    "\n",
    "- Map 'M' to 1 and 'B' to 0 for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc = drop_columns(df_bc, [\"id\"])\n",
    "df_bc[\"diagnosis\"] = df_bc[\"diagnosis\"].map({\"M\": 1, \"B\": 0}).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Handle Missing Values\n",
    "\n",
    " - From `df.info()`, there are no missing values (569 non-null entries per column).\n",
    "\n",
    " - Apply the function for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc = handle_missing_values(df_bc, strategy=\"mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split Features and Target\n",
    "\n",
    " - **Target**: `diagnosis` (now int, binary for classification).\n",
    "\n",
    " - **Features**: All other columns (30 numerical features like `radius_mean`, `texture_mean`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"diagnosis\"\n",
    "X_bc, y_bc = feature_target_split(df_bc, target_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Columns in Features (Safeguard) and Normalize\n",
    "\n",
    "- No categorical columns in features (all are float64 after dropping `id` and encoding `diagnosis`).\n",
    "\n",
    "- Apply the function as a safeguard for future datasets.\n",
    "\n",
    "- Normalize only the feature columns (X) to ensure consistent scale.\n",
    "\n",
    "- Do not normalize the target (`diagnosis`) as it’s a binary label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bc = encode_categorical(X_bc)\n",
    "X_bc = normalize(X_bc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Convert to NumPy Arrays\n",
    "\n",
    " - Convert features and target to NumPy arrays for model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bc = X_bc.to_numpy()\n",
    "y_bc = y_bc.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split into Training and Test Sets and Create Two Versions of the Target\n",
    "\n",
    "\n",
    "\n",
    " The data is split 80/20. Additionally, an SVM version of the target is created by converting 0 to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = split_data(X_bc, y_bc, test_size=0.2)\n",
    "\n",
    "# Create SVM targets by replacing 0 with -1\n",
    "y_train_bc_svm = np.where(y_train_bc == 0, -1, 1)\n",
    "y_test_bc_svm = np.where(y_test_bc == 0, -1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Save Processed Data for Both Logistic Regression and SVM\n",
    "\n",
    "\n",
    "\n",
    " The logistic regression version uses {0,1} targets. The SVM version uses {-1,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save logistic regression data\n",
    "np.save(\"../data/processed/breast_cancer_X_train.npy\", X_train_bc)\n",
    "np.save(\"../data/processed/breast_cancer_X_test.npy\", X_test_bc)\n",
    "np.save(\"../data/processed/breast_cancer_y_train.npy\", y_train_bc)\n",
    "np.save(\"../data/processed/breast_cancer_y_test.npy\", y_test_bc)\n",
    "\n",
    "# Save SVM-specific targets\n",
    "np.save(\"../data/processed/breast_cancer_y_train_svm.npy\", y_train_bc_svm)\n",
    "np.save(\"../data/processed/breast_cancer_y_test_svm.npy\", y_test_bc_svm)\n",
    "\n",
    "print(\"\\nBreast Cancer data (both logistic regression and SVM versions) processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penguins Preprocessing for K-Means with PCA\n",
    "\n",
    "This script preprocesses the Penguins dataset to create a fully numeric dataset,\n",
    "applies normalization, and then reduces its dimensionality using PCA.\n",
    "\n",
    "The resulting data (with `n_components=2`) is ready for training a K-Means clustering model.\n",
    "\n",
    "Steps:\n",
    "1. Load and shuffle the dataset.\n",
    "2. Handle missing values using a \"differentiated\" strategy.\n",
    "3. Encode categorical features to numeric values.\n",
    "4. Normalize all numeric features to the [0, 1] range.\n",
    "5. Apply PCA to reduce dimensionality.\n",
    "6. Save the final dataset for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Penguins dataset\n",
    "file_path = Path(\"../data/raw/K_Means_Dataset/penguins.csv\")\n",
    "df_penguins = load_data(file_path)\n",
    "\n",
    "# Optionally, shuffle the dataset\n",
    "df_penguins = shuffle_data_pandas(df_penguins)\n",
    "\n",
    "# Display basic info and the first few rows\n",
    "print(\"Penguins Dataset Info:\")\n",
    "print(df_penguins.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_penguins.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Handle Missing Values\n",
    "\n",
    "\n",
    "\n",
    " Use the \"differentiated\" strategy to:\n",
    "\n",
    " - Impute missing numeric values with the median.\n",
    "\n",
    " - Impute missing categorical values with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins = handle_missing_values(df_penguins, strategy=\"differentiated\")\n",
    "\n",
    "# Verify that there are no missing values left\n",
    "print(\"Missing values after handling:\")\n",
    "print(df_penguins.isnull().sum())\n",
    "print(df_penguins.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Features\n",
    "\n",
    "Convert all columns of type object or category to numeric using factorization.\n",
    "This ensures the data is fully numeric for K-Means training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins_encoded = encode_categorical(df_penguins)\n",
    "\n",
    "print(\"After encoding categorical features:\")\n",
    "print(df_penguins_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Normalize Features\n",
    "\n",
    "\n",
    "\n",
    " K-Means is sensitive to the scale of features. We normalize the numeric features to the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins_norm = normalize(df_penguins_encoded)\n",
    "print(\"\\nNormalized features sample:\")\n",
    "print(df_penguins_norm.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA to Reduce Dimensionality\n",
    "\n",
    "We use PCA from scikit-learn to reduce the data to two principal components.\n",
    "This step is useful for visualization and to simplify clustering with K-Means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_penguins_norm.to_numpy()\n",
    "\n",
    "# Initialize PCA with desired number of components (e.g., 2)\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "print(\"Data shape after PCA transformation:\", data_pca.shape)\n",
    "print(\"Explained variance ratio by components:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Processed Data\n",
    "\n",
    "- Save the PCA-transformed data as a NumPy array.\n",
    "- This file,\"kmeans_penguins_data.npy\", is now ready to be fed directly into a K-Means model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full normalized features\n",
    "np.save(\"../data/processed/kmeans_penguins_data.npy\", data_pca)\n",
    "\n",
    "print(\"\\nPenguins dataset processed and saved for K-Means clustering. ^__^\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
